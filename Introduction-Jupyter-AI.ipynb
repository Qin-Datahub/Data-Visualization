{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28358d41-849b-4cc4-904f-770daccf6b30",
   "metadata": {},
   "source": [
    "# **Revolutionize Your Workflow: Harnessing the Power of Generative AI and JupyterLab for Rapid Problem-Solving**\n",
    "\n",
    "## About Jupyter AI\n",
    "Jupyter AI brings generative AI to Jupyter. It provides a user-friendly and powerful way to explore generative AI models in notebooks and improve our productivity in JupyterLab and the Jupyter Notebook. According to its official documentation, it supports a wide range of model providers and models such as **OpenAI**, **Hugging Face Hub**, **A21**, **SageMaker**, etc. In order to use Jupyter AI with a particular provider, we have to install its Python packages and set its API key in our environment or in the chat interface:\n",
    "\n",
    "<img src=\"images/interface.png\" width=\"200\">\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "* Install JupyterLab: `pip install jupyterlab~=4.0`\n",
    "  \n",
    "* Start JupyterLab: `!OPENAI_API_KEY=xxxxxxxxxx jupyter lab` (set OpenAI API key environment variable here so we can use Jupyter AI with `%%ai` and `%ai` commands later in the notebook cell)\n",
    "\n",
    "\n",
    "## Dual Modes of Interacting with LLMs\n",
    "\n",
    "### Jupyternaut\n",
    "\n",
    "To engage in conversations with LLMs in a manner similar to OpenAI's ChatGPT, the user-friendly Jupyternaut chat assistant can be employed. The setup process is remarkably simple:\n",
    "\n",
    "1. Access the chat interface: Begin by locating and clicking on the chat icon situated within the left-hand side panel of JupyterLab. This action initiates the chat interface, a gateway to effortless communication through Jupyternaut.\n",
    "2. Start the configuration: Select the `Start Here` option, incorporate your unique OpenAI API Key, and tailor your interaction experience by selecting the preferred language model and embedding model that align with your communication goals. \n",
    "3. Save changes and engage with Jupyternaut:\n",
    "\n",
    "<img src=\"images/chat-getting-started.png\" width=\"200\">\n",
    "<img src=\"images/jupyternaut.png\" width=\"200\">\n",
    "<img src=\"images/example.png\" width=\"200\">\n",
    "\n",
    "**Note:** One thing worth noting is that the language model is ready to be used for responding to our messages while the embedding model is used to transform input data, such as locally-saved text, images, or audio, into a numerical representation that can be processed by other models. This allows Jupyter AI to compose prompts using those representations and send prompts to language models, thus giving us customized answers based on our local documents.\n",
    "\n",
    "In our case, I will use OpenAI's `gpt-3.5-turbo` as the language model and `text-embedding-ada-002` as the embedding model to power my data analysis and visualization process. \n",
    "\n",
    "### Magic Commands\n",
    "\n",
    "Jupyter Notebook magic commands are special commands that provide additional functionality and control with Jupyter Notebook and JupyterLab environments. These commands start with a single `%` sign for line magics (affecting a single line) or `%%` for cell magics (affecting an entire code cell).\n",
    "\n",
    "In order to employ Jupyter AI, we can also use magic commands - `%ai` and `%%ai` within the notebook cells.\n",
    "\n",
    "If you already have `jupyter_ai` installed, the magics package `jupyter_ai_magics` is installed automatically. Otherwise, run `%pip run \n",
    "jupyter_ai_magics` to install the package.\n",
    "\n",
    "Before sending our first prompt to an AI model, load the IPython extension by running: `%load_ext jupyter_ai_magics`.\n",
    "\n",
    "Once the extension has loaded, you can run `%%ai` cell magic command and `%ai` line magic commands. The section below will walk through the basic commands that will help us navigate and manipulate the models.\n",
    "\n",
    "First, let's install some dependencies! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8932d4a0-4d0b-4c84-acc7-1eca1aded402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install openai\n",
    "# %pip install jupyter_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "2a48dfb9-ed99-4018-bbf8-29d768656365",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_ai_magics "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d06fac8-c1ce-4c43-b9e4-8080f3d8e29b",
   "metadata": {},
   "source": [
    "### List available models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "767d5e19-4ac8-4300-a20f-24542c4e3ef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Provider | Environment variable | Set? | Models |\n",
       "|----------|----------------------|------|--------|\n",
       "| `ai21` | `AI21_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | `ai21:j1-large`, `ai21:j1-grande`, `ai21:j1-jumbo`, `ai21:j1-grande-instruct`, `ai21:j2-large`, `ai21:j2-grande`, `ai21:j2-jumbo`, `ai21:j2-grande-instruct`, `ai21:j2-jumbo-instruct` |\n",
       "| `bedrock` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | `bedrock:amazon.titan-tg1-large`, `bedrock:anthropic.claude-v1`, `bedrock:anthropic.claude-instant-v1`, `bedrock:ai21.j2-jumbo-instruct`, `bedrock:ai21.j2-grande-instruct` |\n",
       "| `anthropic` | `ANTHROPIC_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | `anthropic:claude-v1`, `anthropic:claude-v1.0`, `anthropic:claude-v1.2`, `anthropic:claude-instant-v1`, `anthropic:claude-instant-v1.0` |\n",
       "| `cohere` | `COHERE_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | `cohere:medium`, `cohere:xlarge` |\n",
       "| `huggingface_hub` | `HUGGINGFACEHUB_API_TOKEN` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | See https://huggingface.co/models for a list of models. Pass a model's repository ID as the model ID; for example, `huggingface_hub:ExampleOwner/example-model`. |\n",
       "| `openai` | `OPENAI_API_KEY` | <abbr title=\"You have set this environment variable, so you can use this provider's models.\">✅</abbr> | `openai:text-davinci-003`, `openai:text-davinci-002`, `openai:text-curie-001`, `openai:text-babbage-001`, `openai:text-ada-001`, `openai:davinci`, `openai:curie`, `openai:babbage`, `openai:ada` |\n",
       "| `openai-chat` | `OPENAI_API_KEY` | <abbr title=\"You have set this environment variable, so you can use this provider's models.\">✅</abbr> | `openai-chat:gpt-4`, `openai-chat:gpt-4-0314`, `openai-chat:gpt-4-32k`, `openai-chat:gpt-4-32k-0314`, `openai-chat:gpt-3.5-turbo`, `openai-chat:gpt-3.5-turbo-0301` |\n",
       "| `openai-chat-new` | `OPENAI_API_KEY` | <abbr title=\"You have set this environment variable, so you can use this provider's models.\">✅</abbr> | `openai-chat-new:gpt-4`, `openai-chat-new:gpt-4-0314`, `openai-chat-new:gpt-4-32k`, `openai-chat-new:gpt-4-32k-0314`, `openai-chat-new:gpt-3.5-turbo`, `openai-chat-new:gpt-3.5-turbo-0301` |\n",
       "| `sagemaker-endpoint` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | Specify an endpoint name as the model ID. In addition, you must include the `--region_name`, `--request_schema`, and the `--response_path` arguments. For more information, see the documentation about [SageMaker endpoints deployment](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deployment.html) and about [using magic commands with SageMaker endpoints](https://jupyter-ai.readthedocs.io/en/latest/users/index.html#using-magic-commands-with-sagemaker-endpoints). |\n",
       "\n",
       "Aliases and custom commands:\n",
       "\n",
       "| Name | Target |\n",
       "|------|--------|\n",
       "| `gpt2` | `huggingface_hub:gpt2` |\n",
       "| `gpt3` | `openai:text-davinci-003` |\n",
       "| `chatgpt` | `openai-chat:gpt-3.5-turbo` |\n",
       "| `gpt4` | `openai-chat:gpt-4` |\n"
      ],
      "text/plain": [
       "ai21\n",
       "Requires environment variable AI21_API_KEY (not set)\n",
       "* ai21:j1-large\n",
       "* ai21:j1-grande\n",
       "* ai21:j1-jumbo\n",
       "* ai21:j1-grande-instruct\n",
       "* ai21:j2-large\n",
       "* ai21:j2-grande\n",
       "* ai21:j2-jumbo\n",
       "* ai21:j2-grande-instruct\n",
       "* ai21:j2-jumbo-instruct\n",
       "\n",
       "bedrock\n",
       "* bedrock:amazon.titan-tg1-large\n",
       "* bedrock:anthropic.claude-v1\n",
       "* bedrock:anthropic.claude-instant-v1\n",
       "* bedrock:ai21.j2-jumbo-instruct\n",
       "* bedrock:ai21.j2-grande-instruct\n",
       "\n",
       "anthropic\n",
       "Requires environment variable ANTHROPIC_API_KEY (not set)\n",
       "* anthropic:claude-v1\n",
       "* anthropic:claude-v1.0\n",
       "* anthropic:claude-v1.2\n",
       "* anthropic:claude-instant-v1\n",
       "* anthropic:claude-instant-v1.0\n",
       "\n",
       "cohere\n",
       "Requires environment variable COHERE_API_KEY (not set)\n",
       "* cohere:medium\n",
       "* cohere:xlarge\n",
       "\n",
       "huggingface_hub\n",
       "Requires environment variable HUGGINGFACEHUB_API_TOKEN (not set)\n",
       "* See https://huggingface.co/models for a list of models. Pass a model's repository ID as the model ID; for example, `huggingface_hub:ExampleOwner/example-model`.\n",
       "\n",
       "openai\n",
       "Requires environment variable OPENAI_API_KEY (set)\n",
       "* openai:text-davinci-003\n",
       "* openai:text-davinci-002\n",
       "* openai:text-curie-001\n",
       "* openai:text-babbage-001\n",
       "* openai:text-ada-001\n",
       "* openai:davinci\n",
       "* openai:curie\n",
       "* openai:babbage\n",
       "* openai:ada\n",
       "\n",
       "openai-chat\n",
       "Requires environment variable OPENAI_API_KEY (set)\n",
       "* openai-chat:gpt-4\n",
       "* openai-chat:gpt-4-0314\n",
       "* openai-chat:gpt-4-32k\n",
       "* openai-chat:gpt-4-32k-0314\n",
       "* openai-chat:gpt-3.5-turbo\n",
       "* openai-chat:gpt-3.5-turbo-0301\n",
       "\n",
       "openai-chat-new\n",
       "Requires environment variable OPENAI_API_KEY (set)\n",
       "* openai-chat-new:gpt-4\n",
       "* openai-chat-new:gpt-4-0314\n",
       "* openai-chat-new:gpt-4-32k\n",
       "* openai-chat-new:gpt-4-32k-0314\n",
       "* openai-chat-new:gpt-3.5-turbo\n",
       "* openai-chat-new:gpt-3.5-turbo-0301\n",
       "\n",
       "sagemaker-endpoint\n",
       "* Specify an endpoint name as the model ID. In addition, you must include the `--region_name`, `--request_schema`, and the `--response_path` arguments. For more information, see the documentation about [SageMaker endpoints deployment](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deployment.html) and about [using magic commands with SageMaker endpoints](https://jupyter-ai.readthedocs.io/en/latest/users/index.html#using-magic-commands-with-sagemaker-endpoints).\n",
       "\n",
       "\n",
       "Aliases and custom commands:\n",
       "gpt2 - huggingface_hub:gpt2\n",
       "gpt3 - openai:text-davinci-003\n",
       "chatgpt - openai-chat:gpt-3.5-turbo\n",
       "gpt4 - openai-chat:gpt-4\n"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ai list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "58dd8917-eaa8-458b-b669-121002075708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Provider | Environment variable | Set? | Models |\n",
       "|----------|----------------------|------|--------|\n",
       "| `openai-chat` | `OPENAI_API_KEY` | <abbr title=\"You have set this environment variable, so you can use this provider's models.\">✅</abbr> | `openai-chat:gpt-4`, `openai-chat:gpt-4-0314`, `openai-chat:gpt-4-32k`, `openai-chat:gpt-4-32k-0314`, `openai-chat:gpt-3.5-turbo`, `openai-chat:gpt-3.5-turbo-0301` |\n"
      ],
      "text/plain": [
       "openai-chat\n",
       "Requires environment variable OPENAI_API_KEY (set)\n",
       "* openai-chat:gpt-4\n",
       "* openai-chat:gpt-4-0314\n",
       "* openai-chat:gpt-4-32k\n",
       "* openai-chat:gpt-4-32k-0314\n",
       "* openai-chat:gpt-3.5-turbo\n",
       "* openai-chat:gpt-3.5-turbo-0301\n",
       "\n"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or list openai-chat models\n",
    "%ai list openai-chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579e60b6-df82-4b0b-86df-475bfa361632",
   "metadata": {},
   "source": [
    "### Register and update model\n",
    "\n",
    "Using the syntax `%ai register NAME TARGET` to create a new alias to an existing alias's target. The target should be specified using `provider-id:model-id` format. E.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "92cedbc3-c23d-4106-b367-1100128f119c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Registered new alias `mychat`"
      ],
      "text/plain": [
       "Registered new alias `mychat`"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ai register mychat openai:text-davinci-003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "58f09d48-0294-4b7a-812a-87f6bd8e972b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Provider | Environment variable | Set? | Models |\n",
       "|----------|----------------------|------|--------|\n",
       "| `ai21` | `AI21_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | `ai21:j1-large`, `ai21:j1-grande`, `ai21:j1-jumbo`, `ai21:j1-grande-instruct`, `ai21:j2-large`, `ai21:j2-grande`, `ai21:j2-jumbo`, `ai21:j2-grande-instruct`, `ai21:j2-jumbo-instruct` |\n",
       "| `bedrock` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | `bedrock:amazon.titan-tg1-large`, `bedrock:anthropic.claude-v1`, `bedrock:anthropic.claude-instant-v1`, `bedrock:ai21.j2-jumbo-instruct`, `bedrock:ai21.j2-grande-instruct` |\n",
       "| `anthropic` | `ANTHROPIC_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | `anthropic:claude-v1`, `anthropic:claude-v1.0`, `anthropic:claude-v1.2`, `anthropic:claude-instant-v1`, `anthropic:claude-instant-v1.0` |\n",
       "| `cohere` | `COHERE_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | `cohere:medium`, `cohere:xlarge` |\n",
       "| `huggingface_hub` | `HUGGINGFACEHUB_API_TOKEN` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | See https://huggingface.co/models for a list of models. Pass a model's repository ID as the model ID; for example, `huggingface_hub:ExampleOwner/example-model`. |\n",
       "| `openai` | `OPENAI_API_KEY` | <abbr title=\"You have set this environment variable, so you can use this provider's models.\">✅</abbr> | `openai:text-davinci-003`, `openai:text-davinci-002`, `openai:text-curie-001`, `openai:text-babbage-001`, `openai:text-ada-001`, `openai:davinci`, `openai:curie`, `openai:babbage`, `openai:ada` |\n",
       "| `openai-chat` | `OPENAI_API_KEY` | <abbr title=\"You have set this environment variable, so you can use this provider's models.\">✅</abbr> | `openai-chat:gpt-4`, `openai-chat:gpt-4-0314`, `openai-chat:gpt-4-32k`, `openai-chat:gpt-4-32k-0314`, `openai-chat:gpt-3.5-turbo`, `openai-chat:gpt-3.5-turbo-0301` |\n",
       "| `openai-chat-new` | `OPENAI_API_KEY` | <abbr title=\"You have set this environment variable, so you can use this provider's models.\">✅</abbr> | `openai-chat-new:gpt-4`, `openai-chat-new:gpt-4-0314`, `openai-chat-new:gpt-4-32k`, `openai-chat-new:gpt-4-32k-0314`, `openai-chat-new:gpt-3.5-turbo`, `openai-chat-new:gpt-3.5-turbo-0301` |\n",
       "| `sagemaker-endpoint` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | Specify an endpoint name as the model ID. In addition, you must include the `--region_name`, `--request_schema`, and the `--response_path` arguments. For more information, see the documentation about [SageMaker endpoints deployment](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deployment.html) and about [using magic commands with SageMaker endpoints](https://jupyter-ai.readthedocs.io/en/latest/users/index.html#using-magic-commands-with-sagemaker-endpoints). |\n",
       "\n",
       "Aliases and custom commands:\n",
       "\n",
       "| Name | Target |\n",
       "|------|--------|\n",
       "| `gpt2` | `huggingface_hub:gpt2` |\n",
       "| `gpt3` | `openai:text-davinci-003` |\n",
       "| `chatgpt` | `openai-chat:gpt-3.5-turbo` |\n",
       "| `gpt4` | `openai-chat:gpt-4` |\n",
       "| `mychat` | `openai:text-davinci-003` |\n"
      ],
      "text/plain": [
       "ai21\n",
       "Requires environment variable AI21_API_KEY (not set)\n",
       "* ai21:j1-large\n",
       "* ai21:j1-grande\n",
       "* ai21:j1-jumbo\n",
       "* ai21:j1-grande-instruct\n",
       "* ai21:j2-large\n",
       "* ai21:j2-grande\n",
       "* ai21:j2-jumbo\n",
       "* ai21:j2-grande-instruct\n",
       "* ai21:j2-jumbo-instruct\n",
       "\n",
       "bedrock\n",
       "* bedrock:amazon.titan-tg1-large\n",
       "* bedrock:anthropic.claude-v1\n",
       "* bedrock:anthropic.claude-instant-v1\n",
       "* bedrock:ai21.j2-jumbo-instruct\n",
       "* bedrock:ai21.j2-grande-instruct\n",
       "\n",
       "anthropic\n",
       "Requires environment variable ANTHROPIC_API_KEY (not set)\n",
       "* anthropic:claude-v1\n",
       "* anthropic:claude-v1.0\n",
       "* anthropic:claude-v1.2\n",
       "* anthropic:claude-instant-v1\n",
       "* anthropic:claude-instant-v1.0\n",
       "\n",
       "cohere\n",
       "Requires environment variable COHERE_API_KEY (not set)\n",
       "* cohere:medium\n",
       "* cohere:xlarge\n",
       "\n",
       "huggingface_hub\n",
       "Requires environment variable HUGGINGFACEHUB_API_TOKEN (not set)\n",
       "* See https://huggingface.co/models for a list of models. Pass a model's repository ID as the model ID; for example, `huggingface_hub:ExampleOwner/example-model`.\n",
       "\n",
       "openai\n",
       "Requires environment variable OPENAI_API_KEY (set)\n",
       "* openai:text-davinci-003\n",
       "* openai:text-davinci-002\n",
       "* openai:text-curie-001\n",
       "* openai:text-babbage-001\n",
       "* openai:text-ada-001\n",
       "* openai:davinci\n",
       "* openai:curie\n",
       "* openai:babbage\n",
       "* openai:ada\n",
       "\n",
       "openai-chat\n",
       "Requires environment variable OPENAI_API_KEY (set)\n",
       "* openai-chat:gpt-4\n",
       "* openai-chat:gpt-4-0314\n",
       "* openai-chat:gpt-4-32k\n",
       "* openai-chat:gpt-4-32k-0314\n",
       "* openai-chat:gpt-3.5-turbo\n",
       "* openai-chat:gpt-3.5-turbo-0301\n",
       "\n",
       "openai-chat-new\n",
       "Requires environment variable OPENAI_API_KEY (set)\n",
       "* openai-chat-new:gpt-4\n",
       "* openai-chat-new:gpt-4-0314\n",
       "* openai-chat-new:gpt-4-32k\n",
       "* openai-chat-new:gpt-4-32k-0314\n",
       "* openai-chat-new:gpt-3.5-turbo\n",
       "* openai-chat-new:gpt-3.5-turbo-0301\n",
       "\n",
       "sagemaker-endpoint\n",
       "* Specify an endpoint name as the model ID. In addition, you must include the `--region_name`, `--request_schema`, and the `--response_path` arguments. For more information, see the documentation about [SageMaker endpoints deployment](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deployment.html) and about [using magic commands with SageMaker endpoints](https://jupyter-ai.readthedocs.io/en/latest/users/index.html#using-magic-commands-with-sagemaker-endpoints).\n",
       "\n",
       "\n",
       "Aliases and custom commands:\n",
       "gpt2 - huggingface_hub:gpt2\n",
       "gpt3 - openai:text-davinci-003\n",
       "chatgpt - openai-chat:gpt-3.5-turbo\n",
       "gpt4 - openai-chat:gpt-4\n",
       "mychat - openai:text-davinci-003\n"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now let's list all available models again and you will see `mychat` is added to the name-target table\n",
    "%ai list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "77c8ec21-795a-4fbd-ad96-1456643d2832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Updated target of alias `mychat`"
      ],
      "text/plain": [
       "Updated target of alias `mychat`"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update target for self-defined name\n",
    "%ai update mychat openai-chat:gpt-3.5-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "1d895034-f505-4f47-8949-a748eaa231b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Provider | Environment variable | Set? | Models |\n",
       "|----------|----------------------|------|--------|\n",
       "| `ai21` | `AI21_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | `ai21:j1-large`, `ai21:j1-grande`, `ai21:j1-jumbo`, `ai21:j1-grande-instruct`, `ai21:j2-large`, `ai21:j2-grande`, `ai21:j2-jumbo`, `ai21:j2-grande-instruct`, `ai21:j2-jumbo-instruct` |\n",
       "| `bedrock` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | `bedrock:amazon.titan-tg1-large`, `bedrock:anthropic.claude-v1`, `bedrock:anthropic.claude-instant-v1`, `bedrock:ai21.j2-jumbo-instruct`, `bedrock:ai21.j2-grande-instruct` |\n",
       "| `anthropic` | `ANTHROPIC_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | `anthropic:claude-v1`, `anthropic:claude-v1.0`, `anthropic:claude-v1.2`, `anthropic:claude-instant-v1`, `anthropic:claude-instant-v1.0` |\n",
       "| `cohere` | `COHERE_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | `cohere:medium`, `cohere:xlarge` |\n",
       "| `huggingface_hub` | `HUGGINGFACEHUB_API_TOKEN` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | See https://huggingface.co/models for a list of models. Pass a model's repository ID as the model ID; for example, `huggingface_hub:ExampleOwner/example-model`. |\n",
       "| `openai` | `OPENAI_API_KEY` | <abbr title=\"You have set this environment variable, so you can use this provider's models.\">✅</abbr> | `openai:text-davinci-003`, `openai:text-davinci-002`, `openai:text-curie-001`, `openai:text-babbage-001`, `openai:text-ada-001`, `openai:davinci`, `openai:curie`, `openai:babbage`, `openai:ada` |\n",
       "| `openai-chat` | `OPENAI_API_KEY` | <abbr title=\"You have set this environment variable, so you can use this provider's models.\">✅</abbr> | `openai-chat:gpt-4`, `openai-chat:gpt-4-0314`, `openai-chat:gpt-4-32k`, `openai-chat:gpt-4-32k-0314`, `openai-chat:gpt-3.5-turbo`, `openai-chat:gpt-3.5-turbo-0301` |\n",
       "| `openai-chat-new` | `OPENAI_API_KEY` | <abbr title=\"You have set this environment variable, so you can use this provider's models.\">✅</abbr> | `openai-chat-new:gpt-4`, `openai-chat-new:gpt-4-0314`, `openai-chat-new:gpt-4-32k`, `openai-chat-new:gpt-4-32k-0314`, `openai-chat-new:gpt-3.5-turbo`, `openai-chat-new:gpt-3.5-turbo-0301` |\n",
       "| `sagemaker-endpoint` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | Specify an endpoint name as the model ID. In addition, you must include the `--region_name`, `--request_schema`, and the `--response_path` arguments. For more information, see the documentation about [SageMaker endpoints deployment](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deployment.html) and about [using magic commands with SageMaker endpoints](https://jupyter-ai.readthedocs.io/en/latest/users/index.html#using-magic-commands-with-sagemaker-endpoints). |\n",
       "\n",
       "Aliases and custom commands:\n",
       "\n",
       "| Name | Target |\n",
       "|------|--------|\n",
       "| `gpt2` | `huggingface_hub:gpt2` |\n",
       "| `gpt3` | `openai:text-davinci-003` |\n",
       "| `chatgpt` | `openai-chat:gpt-3.5-turbo` |\n",
       "| `gpt4` | `openai-chat:gpt-4` |\n",
       "| `mychat` | `openai-chat:gpt-3.5-turbo` |\n"
      ],
      "text/plain": [
       "ai21\n",
       "Requires environment variable AI21_API_KEY (not set)\n",
       "* ai21:j1-large\n",
       "* ai21:j1-grande\n",
       "* ai21:j1-jumbo\n",
       "* ai21:j1-grande-instruct\n",
       "* ai21:j2-large\n",
       "* ai21:j2-grande\n",
       "* ai21:j2-jumbo\n",
       "* ai21:j2-grande-instruct\n",
       "* ai21:j2-jumbo-instruct\n",
       "\n",
       "bedrock\n",
       "* bedrock:amazon.titan-tg1-large\n",
       "* bedrock:anthropic.claude-v1\n",
       "* bedrock:anthropic.claude-instant-v1\n",
       "* bedrock:ai21.j2-jumbo-instruct\n",
       "* bedrock:ai21.j2-grande-instruct\n",
       "\n",
       "anthropic\n",
       "Requires environment variable ANTHROPIC_API_KEY (not set)\n",
       "* anthropic:claude-v1\n",
       "* anthropic:claude-v1.0\n",
       "* anthropic:claude-v1.2\n",
       "* anthropic:claude-instant-v1\n",
       "* anthropic:claude-instant-v1.0\n",
       "\n",
       "cohere\n",
       "Requires environment variable COHERE_API_KEY (not set)\n",
       "* cohere:medium\n",
       "* cohere:xlarge\n",
       "\n",
       "huggingface_hub\n",
       "Requires environment variable HUGGINGFACEHUB_API_TOKEN (not set)\n",
       "* See https://huggingface.co/models for a list of models. Pass a model's repository ID as the model ID; for example, `huggingface_hub:ExampleOwner/example-model`.\n",
       "\n",
       "openai\n",
       "Requires environment variable OPENAI_API_KEY (set)\n",
       "* openai:text-davinci-003\n",
       "* openai:text-davinci-002\n",
       "* openai:text-curie-001\n",
       "* openai:text-babbage-001\n",
       "* openai:text-ada-001\n",
       "* openai:davinci\n",
       "* openai:curie\n",
       "* openai:babbage\n",
       "* openai:ada\n",
       "\n",
       "openai-chat\n",
       "Requires environment variable OPENAI_API_KEY (set)\n",
       "* openai-chat:gpt-4\n",
       "* openai-chat:gpt-4-0314\n",
       "* openai-chat:gpt-4-32k\n",
       "* openai-chat:gpt-4-32k-0314\n",
       "* openai-chat:gpt-3.5-turbo\n",
       "* openai-chat:gpt-3.5-turbo-0301\n",
       "\n",
       "openai-chat-new\n",
       "Requires environment variable OPENAI_API_KEY (set)\n",
       "* openai-chat-new:gpt-4\n",
       "* openai-chat-new:gpt-4-0314\n",
       "* openai-chat-new:gpt-4-32k\n",
       "* openai-chat-new:gpt-4-32k-0314\n",
       "* openai-chat-new:gpt-3.5-turbo\n",
       "* openai-chat-new:gpt-3.5-turbo-0301\n",
       "\n",
       "sagemaker-endpoint\n",
       "* Specify an endpoint name as the model ID. In addition, you must include the `--region_name`, `--request_schema`, and the `--response_path` arguments. For more information, see the documentation about [SageMaker endpoints deployment](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deployment.html) and about [using magic commands with SageMaker endpoints](https://jupyter-ai.readthedocs.io/en/latest/users/index.html#using-magic-commands-with-sagemaker-endpoints).\n",
       "\n",
       "\n",
       "Aliases and custom commands:\n",
       "gpt2 - huggingface_hub:gpt2\n",
       "gpt3 - openai:text-davinci-003\n",
       "chatgpt - openai-chat:gpt-3.5-turbo\n",
       "gpt4 - openai-chat:gpt-4\n",
       "mychat - openai-chat:gpt-3.5-turbo\n"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ai list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47270d89-25c3-47a2-ad8c-849285ef9e44",
   "metadata": {},
   "source": [
    "Now the target of `mychat` is updated to `openai-chat:gpt-3.5-turbo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "0608f501-91fb-477d-94fd-231fbcc924d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Deleted alias `mychat`"
      ],
      "text/plain": [
       "Deleted alias `mychat`"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete self-defined name\n",
    "%ai delete mychat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50e3e78-5a00-4949-879e-08f2e8ab5594",
   "metadata": {},
   "source": [
    "### Consume registered model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "c0207aaa-1d20-4b02-9db5-0dd144639a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To add two numbers in Python and produce the output in markdown format, you can use the following code:\n",
       "\n",
       "```python\n",
       "num1 = 5\n",
       "num2 = 10\n",
       "\n",
       "sum = num1 + num2\n",
       "\n",
       "print(f\"The sum of {num1} and {num2} is **{sum}**.\")\n",
       "```\n",
       "\n",
       "This code will add the numbers 5 and 10 and output:\n",
       "\n",
       "\"The sum of 5 and 10 is **15**.\"\n",
       "\n",
       "Please note that the markdown formatting is specific to the output format and cannot be achieved in the Python code itself."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 210,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "gpt-3.5-turbo",
        "provider_id": "openai-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai chatgpt\n",
    "Write python code to add two numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8926332e-4066-438c-9a72-d5cdf4d354fd",
   "metadata": {},
   "source": [
    "**Notice**: \n",
    "* If the command above gives a \"Line magic function % not found\" error, try to restart the kernel and rerun the code.\n",
    "* Don't leave any comments in the cell above\n",
    "* The first line specifies the model to use and the second line specifies the message you want to send to the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a32f127-67b7-4840-a6ed-b7a9ed44c77b",
   "metadata": {},
   "source": [
    "### Format the output\n",
    "\n",
    "By default, Jupyter AI assumes that a model will output markdown, but we can override this using `-f` or `--format` argument to our magic command. Valid formats include:\n",
    "\n",
    "* `code`\n",
    "* `image`(for Hugging Face Hub's text-to-image models only)\n",
    "* `markdown`\n",
    "* `math`\n",
    "* `html`\n",
    "* `json`\n",
    "* `text`\n",
    "\n",
    "For example, to force the model to output html format answers, we can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "4e4dece0-31e0-4a5e-9842-a25519ea3c1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "To add two numbers in Python and produce the output in HTML format without any markup before or afterward, you can use the following code:\n",
       "\n",
       "```python\n",
       "num1 = 5\n",
       "num2 = 10\n",
       "\n",
       "sum = num1 + num2\n",
       "\n",
       "print(f\"<h1>The sum of {num1} and {num2} is {sum}.</h1>\")\n",
       "```\n",
       "\n",
       "This code will add the numbers 5 and 10 and output:\n",
       "\n",
       "\"<h1>The sum of 5 and 10 is 15.</h1>\"\n",
       "\n",
       "Please note that the HTML formatting is specific to the output format and cannot be achieved in the Python code itself."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 211,
     "metadata": {
      "text/html": {
       "jupyter_ai": {
        "model_id": "gpt-3.5-turbo",
        "provider_id": "openai-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai chatgpt -f html\n",
    "Write python code to add two numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f12cfc-c70d-4e07-bb6a-aaf9d9d9b3d8",
   "metadata": {},
   "source": [
    "### Interpolating in prompts\n",
    "\n",
    "We can use curly brace syntax to include variables and other Python expressions in our prompts. This lets us execute a prompt using code that the IPython kernel knows about, but that is not in the current cell. E.g., "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "b4a1d932-8770-423f-be79-b757f2d7a5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"Hotel demand analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "c8962e73-17ae-4a2d-9837-f341a1af2125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Certainly! Here's a suggestion for a dashboard title that effectively communicates the purpose of your Hotel demand analysis:\n",
       "\n",
       "**\"Hotel Occupancy Insights: Analyzing Demand Trends for Optimal Strategy and Revenue Generation\"**\n",
       "\n",
       "This title highlights the key aspects of your dashboard, such as analyzing hotel occupancy, understanding demand trends, and leveraging the insights to shape optimal strategy and generate revenue. It conveys the message clearly to your audience, making it easy for them to understand the kind of information you aim to present. Feel free to modify it according to your specific analysis and target audience."
      ]
     },
     "execution_count": 213,
     "metadata": {
      "jupyter_ai": {
       "model_id": "gpt-3.5-turbo",
       "provider_id": "openai-chat"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai chatgpt -f text\n",
    "I want to build a dashboard presenting {topic}. I want you to come up with a dashboard title for me so it's easy for other people to understand what kind of information I want to convey to my audience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06aa5c6",
   "metadata": {},
   "source": [
    "### Error/Code explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "6c9230bd-69f0-4cbd-826f-d2f2bb17ffe5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[214], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# error explanation\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# error explanation\n",
    "2/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "9047bec9-f0a9-41a5-b208-e69385c0e73e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The error that occurred is a ZeroDivisionError. It is raised when attempting to divide a number by zero, which is mathematically undefined. \n",
       "\n",
       "In the code snippet provided, the expression \"2/0\" attempts to divide the number 2 by zero. This operation is not possible and results in a ZeroDivisionError. \n",
       "\n",
       "The error message indicates \"ZeroDivisionError: division by zero,\" informing you about the nature of the error.\n",
       "\n",
       "Please note that the markdown formatting is specific to the output format and cannot be achieved within Python code itself."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 215,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "gpt-3.5-turbo",
        "provider_id": "openai-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai chatgpt \n",
    "Explain error {Err[214]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "0516ed5c-fe5e-445b-b774-8f9ccba1b0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qin Yang\n"
     ]
    }
   ],
   "source": [
    "# code explanation\n",
    "print(\"Qin Yang\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "bd9893e3-0555-470d-a9e0-5e64ee2896b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Cannot determine model provider from model ID `mychat`.\n",
       "\n",
       "To see a list of models you can use, run `%ai list`\n",
       "\n",
       "If you were trying to run a command, run `%ai help` to see a list of commands."
      ],
      "text/plain": [
       "Cannot determine model provider from model ID 'mychat'.\n",
       "\n",
       "To see a list of models you can use, run '%ai list'\n",
       "\n",
       "If you were trying to run a command, run '%ai help' to see a list of commands."
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai mychat\n",
    "Explain code {In[216]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc07e0b-a17a-4f62-b34a-c475836289bb",
   "metadata": {},
   "source": [
    "### Clear chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "66b6a325-e16c-47c8-b0d8-062d39b8fa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ai chatgpt -r\n",
    "reset the chat history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
